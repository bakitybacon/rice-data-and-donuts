---
title: "Analysis of Uber Pickups Data from January to June 2015"
author: "Corrin Fosmire"
date: "7/30/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~")
options(warn=-1)
library(tidyverse)
library(lubridate)
library(tis)
library(timeDate)
library(forecast)
library(tseries)
library(prophet)
library(keras)
library(tensorflow)
```

## Importing Data

```{r}
pickups <- read_csv("uber-raw-data-janjune-15.csv")
daydata <- pickups %>%
  mutate(day=yday(Pickup_date)) %>%
  group_by(day) %>%
  summarize(count=n())
```

## Exploratory Data Analysis

Plotting day-by-day, by month, day of month, day of week, and by hour.


```{r}
daydata %>%
  ggplot() +
  geom_point(mapping=aes(day, count)) +
  geom_line(mapping=aes(day, count)) +
  labs(title="Uber Pickups by Day of Year between January and June 2015", y="Pickup Count", x="Day")
```

```{r}
pickups %>%
  mutate(month=month(Pickup_date)) %>%
  group_by(month) %>%
  summarize(count=n()) %>%
  ggplot() +
  geom_point(mapping=aes(month, count)) +
  geom_line(mapping=aes(month, count))  +
  labs(title="Uber Pickups by Month between January and June 2015",  y="Pickup Count", x="Month")
```

```{r}
pickups %>%
  mutate(day=day(Pickup_date)) %>%
  group_by(day) %>%
  summarize(count=n()) %>%
  ggplot() +
  geom_point(mapping=aes(day, count)) +
  geom_line(mapping=aes(day, count))  +
  labs(title="Uber Pickups by Day of Month between January and June 2015",  y="Pickup Count", x="Day of Month")
```

```{r}
pickups %>%
  mutate(hour=hour(Pickup_date)) %>%
  group_by(hour) %>%
  summarize(count=n()) %>%
  ggplot() +
  geom_point(mapping=aes(hour, count)) +
  geom_line(mapping=aes(hour, count))  +
  labs(title="Uber Pickups by Hour of Day between January and June 2015",  y="Pickup Count", x="Hour")
```
```{r}
pickups %>%
  mutate(wday=wday(Pickup_date)) %>%
  group_by(wday) %>%
  summarize(count=n()) %>%
  ggplot() +
  geom_point(mapping=aes(wday, count)) +
  geom_line(mapping=aes(wday, count))  +
  labs(title="Uber Pickups by Hour of Day between January and June 2015",  y="Pickup Count", x="Day of Week")
```

Based on these plots, it looks like day of month is not likely to be significant, as it does not display any particular pattern. Hour, month, and day of week seem to be important.

Now, a fun little joint graph, checking how the relationship between hour and number of pickups changes with the day of the week.

```{r}
pickups %>%
  mutate(wday=wday(Pickup_date, label=TRUE, abbr=TRUE)) %>%
  mutate(hour=hour(Pickup_date)) %>%
  group_by_at(vars(wday, hour)) %>%
  summarize(count=n()) %>%
  ggplot() +
  geom_point(mapping=aes(hour, count)) +
  geom_line(mapping=aes(hour, count, color=wday))  +
  labs(title="Uber Pickups by Hour of Day between January and June 2015",  y="Pickup Count", x="Hour of Day",
       color="Day of Week")
```

Naturally, Friday and Saturday nights have the highest pickup rates, as all the drunken revelers stagger back to their apartments.

Now, onto a little exploration of how holidays affect Uber pickups.

Here are the holidays I'm going to use:

```{r}
some_holidays <- tribble(
  ~name, ~datestr,
  "New Year's Day", "20150101",
  "MLK Day", "20150119",
  "Valentine's Day", "20150214",
  "Presidents' Day", "20150216",
  "Memorial Day", "20150525"
)
some_holidays$date <- as_date(some_holidays$datestr)
some_holidays
```

Now, let's see what the average pickups per day is for holidays and non-holidays.

```{r}
holidays <- pickups %>%
  mutate(date=as_date(Pickup_date)) %>%
  filter(date %in% as_date(some_holidays$date)) %>%
  mutate(day=yday(Pickup_date)) %>%
  group_by(day) %>%
  summarize(total=n())

notholidays <- pickups %>%
  mutate(date=as_date(Pickup_date)) %>%
  filter(!(date %in% as_date(some_holidays$date))) %>%
  mutate(day=yday(Pickup_date)) %>%
  group_by(day) %>%
  summarize(total=n())

holidaysvsnot <- tribble(
  ~holidays, ~pickupsperday,
  "Yes", sum(holidays$total) / nrow(holidays),
  "No", sum(notholidays$total) / nrow(notholidays)
)

holidaysvsnot %>%
  ggplot() +
  geom_col(mapping=aes(holidays, pickupsperday)) +
  labs(y="Pickups Per Day")
```

It looks like people use Uber more on holidays than not. Probably, holidays aren't as important as other factors, or people are just staying home with family on holidays.

## Checking out the trend

We first fit a simple line of best fit, just to give us an idea how the mean is changing over time.

```{r}
linearmod <- lm("count ~ day", daydata)
linearfit <- predict.lm(linearmod)
```

```{r}
daydata %>%
  ggplot() +
  geom_point(mapping=aes(day, count)) +
  geom_line(mapping=aes(day, count)) +
  geom_line(mapping=aes(day, linearfit), color="red") +
  labs(title="Uber Pickups by Day between January and June 2015 with Trend",  y="Pickup Count", x="Day")
```

Interestingly, it looks like Uber is steadily rising in usage throughout 2015.

```{r}
daydata %>%
  mutate(detrended=count-linearfit) %>%
  ggplot() +
  geom_point(mapping=aes(day, detrended)) +
  geom_line(mapping=aes(day, detrended)) +
  labs(title="Uber Pickups by Day between January and June 2015, Detrended",  y="Pickup Count", x="Day")
```

Here's what it looks like if we ignore the trend and focus on the other deviations.

However, detrending has a problem: is the true trend even linear? It's better to use differencing to accomplish the same goals, as it doesn't make any assumptions about the trend that could be completely wrong.

## Differencing

Differencing means subtracting previous values, one or more steps back. If it's one step, it will remove a linear trend. If it's a larger number, like seven, it can remove seasonalities, such as weekly seasonality here.

Let's start by differencing by 1 and 7. Notice how differencing by 1 looks similar to our detrended plot, in that the series is now fluctuating around zero. Also, it seems that there's a good degree of cyclical patterns in the 7th order differenced series, so we shouldn't just stop there.

```{r}
difforders = c(1, 7)

difference_plot <- function(order) {
  daydata %>%
    mutate(differenced=count-lag(count, order)) %>%
    drop_na() %>%
    ggplot() +
    geom_point(mapping=aes(day, differenced)) +
    geom_line(mapping=aes(day, differenced)) +
    labs(title=str_c("Difference Order: ", as.character(order)))
}

difforders %>%
  map(difference_plot)
```

Now, let's try combining the two.

```{r}
daydata %>%
  mutate(differenced=count-lag(count, 1)) %>%
  drop_na() %>%
  mutate(differenced=differenced-lag(differenced, 7)) %>%
  drop_na() %>%
  ggplot() +
  geom_point(mapping=aes(day, differenced)) +
  geom_line(mapping=aes(day, differenced)) +
  labs(title="Difference Orders: 1 and 7")
```

Let's use the Augmented Dickey-Fuller test to see how well we did.

```{r}
daydata %>%
  mutate(differenced=count-lag(count, 7)) %>%
  drop_na() %>%
  mutate(differenced=differenced-lag(differenced)) %>%
  drop_na() %>%
  pull(differenced) %>%
  adf.test()
```

Because our p-value is so small, the Augmented Dickey-Fuller test indicates we are stationary!

## Lagged Scatterplots

How do we actually find good orders for differencing? A good way is to use lagged scatterplots.

The idea is we're plotting a point versus a previous point, either the previous one or seven back, in the case of weeks, for example.

Let's look at a couple for our Uber data. If one lag value has a strong linear pattern with a fairly steep positive or negative slope, we should apply differencing with that lag value.

```{r}
lag_scatter_map <- function(datavector, maxlag) {
  1:maxlag %>%
    map(~ lag_scatter(datavector, .x))
}

lag_scatter <- function(datavector, laglevel) {
  tibble(datavec=datavector, datalag=lag(datavector, laglevel)) %>%
    drop_na() %>%
    ggplot() +
    geom_point(mapping=aes(datavec, datalag)) +
    geom_smooth(mapping=aes(datavec, datalag), method = "loess", size = 1.5) +
    labs(y=str_c("Lagged Pickups of Order ",as.character(laglevel)),
         x="Day of Year")
}

lag_scatter_map(daydata$count, 9)
```

As expected, one, seven, and eight have the strongest, steepest linear trend.

## Moving averages

Let's take a look at another kind of pre-processing step. A moving average averages in a small window of the data set, say for example three data points. It's used to smooth the data to diminish the effect of huge spikes. Note that a Moving Average of order one is simply the original series.

```{r}
maorders = c(1, 3, 5, 7)

ma_plot <- function(order) {
  daydata %>%
    mutate(ma=ma(count, order, centre=FALSE)) %>%
    drop_na() %>%
    ggplot() +
    geom_point(mapping=aes(day, ma)) +
    geom_line(mapping=aes(day, ma)) +
    labs(title=str_c("Moving Average Difference Order: ", as.character(order)))
}

maorders %>%
  map(ma_plot)
```

Notice this does not de-trend the data. It does, however, make it a little smoother for modeling. There is a bit of a tradeoff with losing information, though!

## ARIMA Models

An ARIMA model contains three components: autoregressive components, moving average components, and a differencing order.

Autoregressive components help explain how much a data point depends upon previous data points.

Moving average components help us not get too carried away by rare, huge deviations.

In order to give us a notion of how many autoregressive components we need, we look at the autocorrelation plot.

```{r}
confbound <- qnorm((1 + 0.95)/2)/sqrt(nrow(daydata))

acf_vector <- as.vector(acf(daydata$count, lag.max = 33, plot=FALSE)$acf)
acftibble <- tibble(lag=0:(length(acf_vector)-1), acf=acf_vector)
acftibble %>%
  mutate(minconf=-confbound) %>%
  mutate(maxconf=confbound) %>%
  ggplot() +
  geom_bar(stat="identity", mapping=aes(lag, acf)) +
  geom_ribbon(mapping=aes(x=lag, ymin=minconf, ymax=maxconf), fill="blue", alpha=0.2) +
  labs(title="Autocorrelation Plot")
```

It looks like at a lag of about thirty the autocorrelation function is in the confidence interval, so we'll start modeling there.

Now, let's do something similar with the partial autocorrelation plot. This will give us an idea of how many moving average components we need.

```{r}
pacf_vector <- as.vector(pacf(daydata$count, plot=FALSE)$acf)
pacftibble <- tibble(lag=0:(length(pacf_vector)-1), pacf=pacf_vector)
pacftibble %>%
  mutate(minconf=-confbound) %>%
  mutate(maxconf=confbound) %>%
  ggplot() +
  geom_bar(stat="identity", mapping=aes(lag, pacf)) +
  geom_ribbon(mapping=aes(x=lag, ymin=minconf, ymax=maxconf), fill="blue", alpha=0.2) +
  labs(title="Partial Autocorrelation Plot")
```

Here, after about seven, all lag values are within the confidence interval. Let's use that for now.

Let's build that model!

First, split the data into two sections. The first section is used to build the model. The second is used to evaluate its effectiveness. 

The first section is commonly called the _train set_. The second is commonly called the _test set_.

```{r}
daydata_train <- daydata %>%
  filter(day < 150)

daydata_test <- daydata %>%
  filter(day >= 150)
```

Now, let's build the model and plot our forecast.

```{r}
arimamodel <- arima(daydata_train$count, order=c(29, 1, 7))
arimafit <- as.vector(forecast(arimamodel, h=32))

ggplot() +
  geom_point(mapping=aes(daydata$day, daydata$count)) +
  geom_line(mapping=aes(daydata$day, daydata$count)) +
  geom_point(mapping=aes(daydata_test$day, arimafit$mean), color="red") +
  geom_line(mapping=aes(daydata_test$day, arimafit$mean), color="red") +
  geom_ribbon(mapping=aes(x=daydata_test$day, ymin=as.vector(arimafit$lower[,1]), ymax=as.vector(arimafit$upper[,1])), color="gray", fill="red", alpha=0.2) +
  geom_vline(mapping=aes(xintercept=150), color="blue") + 
  labs(title="A Fitted Arima Model", y="Pickup Count", x="Day")
```

Not too bad!

Let's look at our mean absolute percentage error (MAPE), an indication of how far off we are, percentage-wise, from the true value.

```{r}
arimamape <- mean(abs(daydata_test$count - arimafit$mean)/daydata_test$count)
arimamape * 100
```

So, we're within about 10% of the true value on average. Can we do better?

## Prophet Models

The prophet library, produced by researchers at Facebook, produced a new class of models.

Some of its features are its flexibility and great support for holidays and seasonality. It uses changepoints to split the series into several smaller components, for each of which it can fit a model individually.

Unfortunately, prophet has some annoying conventions. Our date column must be named "ds". The variable we are predicting must be named "y".

```{r}
prophet_train <- daydata_train
prophet_test <- daydata_test

colnames(prophet_train) <- c("ds","y")
colnames(prophet_test) <- c("ds","y")

day1 <- as_date("20150101")
prophet_train$ds <- day1 + days(prophet_train$ds - 1)
prophet_test$ds <- day1 + days(prophet_test$ds - 1)
```

Let's take the model out for a spin.

```{r}
prophetmodel <- prophet(prophet_train, fit=TRUE, daily.seasonality = TRUE, weekly.seasonality = TRUE)
prophetfit <- predict(prophetmodel, prophet_test)

ggplot() +
  geom_point(mapping=aes(daydata$day, daydata$count)) +
  geom_line(mapping=aes(daydata$day, daydata$count)) +
  geom_point(mapping=aes(daydata_test$day, prophetfit$yhat), color="red") +
  geom_line(mapping=aes(daydata_test$day, prophetfit$yhat), color="red") +
  geom_ribbon(mapping=aes(x=daydata_test$day, ymin=prophetfit$yhat_lower, ymax=prophetfit$yhat_upper), color="gray", fill="red", alpha=0.2) +
  geom_vline(mapping=aes(xintercept=150), color="blue") + 
  labs(title="A Fitted Prophet Model", y="Pickup Count", x="Day")
```
Looks pretty good!

```{r}
prophetmape <- mean(abs(daydata_test$count - prophetfit$yhat)/daydata_test$count)
prophetmape * 100
```

Now we're within 7.5% of the true value on average!

Let's investigate the model a little more.

```{r}
# Seasonal decompose of prophet
prophet_plot_components(prophetmodel, prophetfit)
```

So, daily and hourly seasonality are being handled as we would expect. It also identifies a solid trend. Notice our model predicts that Friday and Saturday have about 30000 more pickups than Monday!

## Neural network approaches

```{r include=FALSE}
tf$InteractiveSession()
tf$GPUOptions$allow_growth <- TRUE
```

To use our data set in tensorflow, we must first rearrange it a little.

```{r}
tf_train_x <- array_reshape(as.array(daydata_train$day), c(nrow(daydata_train), 1, 1))
tf_train_y <- array_reshape(as.array(daydata_train$count), c(nrow(daydata_train), 1, 1))
tf_test_x <- array_reshape(as.array(daydata_test$day), c(nrow(daydata_test), 1, 1))
tf_test_y <- array_reshape(as.array(daydata_test$count), c(nrow(daydata_test), 1, 1))
```

Now, we can fit a Simple RNN to our data.

```{r}
rnn <- keras_model_sequential()
rnn %>%
  layer_simple_rnn(1, return_sequences=TRUE, input_shape = c(1, 1)) %>%
  layer_activation_relu() %>%
  layer_dense(1)

rnn %>%
  compile(
    optimizer = "adam",
    loss = "mean_absolute_percentage_error"
  )

rnn %>% 
  fit(
    tf_train_x, tf_train_y,
    epochs = 30,
    validation_split = 0.2,
    verbose=FALSE
  )
```

Let's see how we did.

```{r}
rnn %>%
  evaluate(tf_test_x, tf_test_y)

rnnfit <- rnn %>%
  predict(tf_test_x)

ggplot() +
  geom_point(mapping=aes(daydata$day, daydata$count)) +
  geom_line(mapping=aes(daydata$day, daydata$count)) +
  geom_point(mapping=aes(daydata_test$day, as.vector(rnnfit)), color="red") +
  geom_line(mapping=aes(daydata_test$day, as.vector(rnnfit)), color="red") +
  geom_vline(mapping=aes(xintercept=150), color="blue") + 
  labs(title="A Fitted Simple RNN Model", y="Pickup Count", x="Day")
```


Completely awful. We simply don't have enough data. Note that I used exactly as much data for this model as I did for the earlier models, which performed quite strongly!

The story is identical for a very simple LSTM architecture.

```{r}
lstm <- keras_model_sequential()
lstm %>%
  layer_lstm(units=1, return_sequences=TRUE, input_shape = c(1, 1)) %>%
  layer_activation_relu() %>%
  layer_dense(1)

lstm %>%
  compile(
    optimizer = "adam",
    loss = "mean_absolute_percentage_error"
  )

lstm %>% 
  fit(
    tf_train_x, tf_train_y,
    epochs = 30,
    validation_split = 0.2,
    verbose=FALSE
  )

lstm %>%
  evaluate(tf_test_x, tf_test_y)

lstmfit <- lstm %>%
  predict(tf_test_x)

ggplot() +
  geom_point(mapping=aes(daydata$day, daydata$count)) +
  geom_line(mapping=aes(daydata$day, daydata$count)) +
  geom_point(mapping=aes(daydata_test$day, as.vector(lstmfit)), color="red") +
  geom_line(mapping=aes(daydata_test$day, as.vector(lstmfit)), color="red") +
  geom_vline(mapping=aes(xintercept=150), color="blue") + 
  labs(title="A Fitted LSTM Model", y="Pickup Count", x="Day")
```


Let's try using more of our available data, doing a few preprocessing steps, and improving our architecture.

First, extract pickups by minute between January and June.

```{r}
minutedata <- pickups %>%
  mutate(day=yday(Pickup_date)) %>%
  mutate(hour=hour(Pickup_date)) %>%
  mutate(minute=minute(Pickup_date)) %>%
  group_by_at(vars(day, hour, minute)) %>%
  summarize(count=n()) %>%
  select(count, day, hour, minute)
```

Now, let's preprocess the data.

Step 1: Filling in gaps so that all minutes are accounted for, even if nobody got picked up. Then, the data is evenly spaced.
Step 2: Differencing by 1 and 7
Step 3: 0-1 scaling

This will make it easier to train with.

```{r results='hide', warning='hide'}
zeroedtbl <- as_tibble(expand.grid(day=1:181, hour=0:23, minute=0:59))

minutedata <- zeroedtbl %>%
  left_join(minutedata, by=c("day", "hour", "minute"))
minutedata$count[is.na(minutedata$count)] <- 0

minutedata <- minutedata %>%
  arrange(day, hour, minute)

minutedata <- minutedata %>%
  mutate(count=count - lag(count)) %>%
  mutate(count=count - lag(count, 7)) %>%
  drop_na()

scaleminterm <- min(minutedata$count)
minutedata <- minutedata %>%
  mutate(count=count-scaleminterm)

scalemaxterm <- max(minutedata$count)
minutedata <- minutedata %>%
  mutate(count=count/scalemaxterm)
```

Let's fix up the indices so our date column is the number of minutes since New Year's and perform our usual train and test split.

```{r}
minutedata <- rowid_to_column(minutedata)

minutedata_train <- minutedata %>%
  filter(rowid < 150*24*60)

minutedata_test <- minutedata %>%
  filter(rowid >= 150*24*60)
```

And, ready our data for tensorflow.

```{r}
min_train_x <- array_reshape(as.array(minutedata_train$rowid), c(nrow(minutedata_train), 1, 1))
min_train_y <- array_reshape(as.array(minutedata_train$count), c(nrow(minutedata_train), 1, 1))
min_test_x <- array_reshape(as.array(minutedata_test$rowid), c(nrow(minutedata_test), 1, 1))
min_test_y <- array_reshape(as.array(minutedata_test$count), c(nrow(minutedata_test), 1, 1))
```

```{r}
biglstm <- keras_model_sequential()
biglstm %>%
  layer_lstm(units=100, return_sequences=TRUE, input_shape = c(1, 1)) %>%
  layer_activation_relu() %>%
  layer_lstm(units=100, return_sequences=TRUE) %>%
  layer_activation_relu() %>%
  layer_lstm(units=50, return_sequences=TRUE) %>%
  layer_activation_relu() %>%
  layer_dense(300) %>%
  layer_activation_relu() %>%
  layer_dense(50) %>%
  layer_activation_relu() %>%
  layer_dense(1)

biglstm %>%
  compile(
    optimizer = "adam",
    loss = "mean_absolute_percentage_error"
  )

biglstm %>% 
  fit(
    min_train_x, min_train_y,
    epochs = 1,
    validation_split = 0.1,
    shuffle=FALSE
  )
```

Let's fix our data back up. 

First, we need to undo the preprocessing. 

```{r}
biglstmfit <- biglstm %>%
  predict(min_test_x)

# lags
biglstmfit <- biglstmfit + lead(biglstmfit, 7)
biglstmfit <- biglstmfit + lead(biglstmfit)

#0-1 scaling
biglstmfit <- biglstmfit * scalemaxterm + scaleminterm
```

Right now our data is in minutes, but we'd rather have it in the form of days, to compare apples to apples.

```{r}
biglstmdates <- day1 + minutes(min_test_x)
biglstmtbl <- tibble(date=as.POSIXct(biglstmdates), count=as.vector(biglstmfit))
biglstmtbl <- biglstmtbl %>%
  mutate(day=yday(date)) %>%
  group_by(day) %>%
  summarize(count=sum(count))
```

And, we're good to go!

```{r}
ggplot() +
  geom_point(mapping=aes(daydata$day, daydata$count)) +
  geom_line(mapping=aes(daydata$day, daydata$count)) +
  geom_point(mapping=aes(biglstmtbl$day, biglstmtbl$count), color="red") +
  geom_line(mapping=aes(biglstmtbl$day, biglstmtbl$count), color="red") +
  geom_vline(mapping=aes(xintercept=150), color="blue") + 
  labs(title="A Fitted LSTM Model", y="Pickup Count", x="Day")
```

Taking a look at our MAPE:

```{r}
after <- subset(daydata_test, day >= 151)
biglstmmape <- mean(abs(after$count - biglstmtbl$count)/as.numeric(after$count), na.rm=TRUE)
biglstmmape * 100
```

Great!